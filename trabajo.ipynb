{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "from typing import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Accuracy\n",
    "def accuracy(M):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(M)):\n",
    "        for j in range(len(M)):\n",
    "            total += M[i][j]\n",
    "            if i == j:\n",
    "                correct+=M[i][j]\n",
    "    return correct/total\n",
    "\n",
    "# Precision\n",
    "def precision(M, index): #index es la posicion de la variable de interes\n",
    "    correct = M[index][index]\n",
    "    total = 0\n",
    "    for i in range(len(M)):\n",
    "        for j in range(len(M)):\n",
    "            if i == index:\n",
    "                total+= M[i][j]\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "# Recall\n",
    "def recall(M, index): #index es la posicion de la variable de interes\n",
    "    correct = M[index][index]\n",
    "    total = 0\n",
    "    for i in range(len(M)):\n",
    "        for j in range(len(M)):\n",
    "            if j == index:\n",
    "                total+= M[i][j]\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "# F1\n",
    "def F1_score(M,index):\n",
    "    P = precision(M,index)\n",
    "    R = recall(M,index)\n",
    "    if P+R == 0:\n",
    "        return 0\n",
    "    return (2*P*R)/(P+R)\n",
    "\n",
    "def macro_avg(func, M): #func es la metrica que queremos usar\n",
    "    accum = 0\n",
    "    for i in range(len(M)):\n",
    "        accum+=func(M,i)\n",
    "    return accum/len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dataset.txt\", \"r\")\n",
    "o = open(\"processed.txt\", \"w\")\n",
    "printable = set(string.printable)\n",
    "to_delete = ['<', '/', '|', '(', '+', '%', ':', '{', '}', '.', ',']\n",
    "\n",
    "bracketsOpen = False\n",
    "for line in f:\n",
    "    line = line.replace(\" - \", \"-\")\n",
    "    for word in line.split():\n",
    "        word = ''.join(filter(lambda x: x in printable, word))\n",
    "        word = word.replace('=', ' ')\n",
    "        word = word.replace('>', ' ')\n",
    "        word = word.replace(')', ' ')\n",
    "\n",
    "        for letter in to_delete:\n",
    "            word = word.replace(letter, '')\n",
    "        w = word.lower()\n",
    "        o.write(w + \" \")\n",
    "f.close()\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "bag = collections.defaultdict(int)\n",
    "f = open(\"processed.txt\", \"r\")\n",
    "for line in f:\n",
    "    for word in line.split():\n",
    "        if word in stop:\n",
    "            continue\n",
    "        bag[word] += 1\n",
    "bag = dict(sorted(bag.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"cells\"\n",
    "pageSize = 500\n",
    "\n",
    "frequent_words = list(bag.keys())\n",
    "feature_index = frequent_words.index(feature)\n",
    "incidence_vector = [0] * len(frequent_words)\n",
    "pages = []\n",
    "pages.append('')\n",
    "f = open(\"processed.txt\", \"r\")\n",
    "for line in f:\n",
    "    for word in line.split():\n",
    "        pages[-1] += \" \" + word\n",
    "        if len(pages[-1].split()) >= pageSize:\n",
    "            pages.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "pageVectors = []\n",
    "\n",
    "for page in pages:\n",
    "    vector = copy.deepcopy(incidence_vector)\n",
    "    for word in page.split():\n",
    "        for i in range(len(frequent_words)):\n",
    "            if word == frequent_words[i]:\n",
    "                vector[i] += 1\n",
    "    pageVectors.append(vector)\n",
    "print(len(pageVectors))\n",
    "# vamos a separar la variable clase\n",
    "clases = []\n",
    "for page in pageVectors:\n",
    "    clase = page[feature_index]\n",
    "    clases.append(clase)\n",
    "    page.pop(feature_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 80\n",
    "\n",
    "# Casteamos las variables a arreglos de numpy:\n",
    "index_percent = int(len(pageVectors)*split/100)\n",
    "X = np.array(pageVectors[:index_percent])\n",
    "Y = np.array(clases[:index_percent])\n",
    "X_test = np.array(pageVectors[index_percent:])\n",
    "Y_test = np.array(clases[index_percent:])\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "pred = clf.predict(X_test)\n",
    "tupla = {}\n",
    "confusion = confusion_matrix(Y_test, pred)\n",
    "tupla[\"acc\"] = round(accuracy(confusion)*100)\n",
    "tupla[\"prec\"] = round(macro_avg(precision, confusion)*100)\n",
    "tupla[\"rec\"] = macro_avg(recall, confusion)*100\n",
    "tupla[\"F1\"] = macro_avg(F1_score, confusion)*100\n",
    "scores.append(tupla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "pred = clf.predict(X_test)\n",
    "tupla = {}\n",
    "confusion = confusion_matrix(Y_test, pred)\n",
    "tupla[\"acc\"] = round(accuracy(confusion)*100)\n",
    "tupla[\"prec\"] = round(macro_avg(precision, confusion)*100)\n",
    "tupla[\"rec\"] = macro_avg(recall, confusion)*100\n",
    "tupla[\"F1\"] = macro_avg(F1_score, confusion)*100\n",
    "scores.append(tupla)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test)\n",
    "pred = clf.predict(X_test)\n",
    "tupla = {}\n",
    "confusion = confusion_matrix(Y_test, pred)\n",
    "tupla[\"acc\"] = round(accuracy(confusion)*100)\n",
    "tupla[\"prec\"] = round(macro_avg(precision, confusion)*100)\n",
    "tupla[\"rec\"] = macro_avg(recall, confusion)*100\n",
    "tupla[\"F1\"] = macro_avg(F1_score, confusion)*100\n",
    "scores.append(tupla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X, Y)\n",
    "reg.score(X, Y)\n",
    "pred = clf.predict(X_test)\n",
    "tupla = {}\n",
    "confusion = confusion_matrix(Y_test, pred)\n",
    "tupla[\"acc\"] = round(accuracy(confusion)*100)\n",
    "tupla[\"prec\"] = round(macro_avg(precision, confusion)*100)\n",
    "tupla[\"rec\"] = macro_avg(recall, confusion)*100\n",
    "tupla[\"F1\"] = macro_avg(F1_score, confusion)*100\n",
    "scores.append(tupla)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    acc  prec        rec        F1\n",
       "0  13.0   9.0   3.671329  4.870130\n",
       "1  22.0   9.0  10.069444  8.838384\n",
       "2  13.0   7.0   4.848485  5.555556\n",
       "3  13.0   7.0   4.848485  5.555556"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc</th>\n      <th>prec</th>\n      <th>rec</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13.0</td>\n      <td>9.0</td>\n      <td>3.671329</td>\n      <td>4.870130</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.0</td>\n      <td>9.0</td>\n      <td>10.069444</td>\n      <td>8.838384</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.0</td>\n      <td>7.0</td>\n      <td>4.848485</td>\n      <td>5.555556</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.0</td>\n      <td>7.0</td>\n      <td>4.848485</td>\n      <td>5.555556</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "marco = pd.DataFrame(scores)\n",
    "marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}